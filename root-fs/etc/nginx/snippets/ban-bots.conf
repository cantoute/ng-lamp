
location = /robots.txt {
  add_header Cache-Control "no-cache, must-revalidate";
  add_header X-Robots-Tag "noindex, follow";
  return 200 "#Ban all bots\nUser-agent: *\nDisallow: /\n";
}

# deny access to bots
if ( $http_user_agent ~* "(bot|crawl|spider|scrap|index|archive|slurp)" ) {
  return 403;
}